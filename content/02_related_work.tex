\chapter{Related Work}
\label{cha:relatedwork}

Web tracking is a pervasive phenomenon on the internet, where users are 
often unaware of the extent to which their online activities are monitored
and recorded by third-party trackers. The use of tracking technologies such
as cookies, pixels, and fingerprinting can allow trackers to collect large 
amounts of personal information about users, including their browsing history,
demographics, and online behavior, without their explicit consent or knowledge 
\cite{englehardt2016online,hoofnagle2010different}. This can lead to a range of
negative consequences for users, including invasion of privacy, identity theft,
and discrimination \cite{hannak2013measuring,narayanan2010myths}.

With increasing presence of web trackers on the Internet, the need for effective
web tracker blocking tools has become more apparent. This need has been recognized
by both the public and various organizations, as evidenced by the popularity of ad
blockers and security browsers such as \textit{Brave}, as well as secure search engines
like \textit{StartPage}. These tools provide users with a greater control over their
online privacy and security, which is especially important in today's age of increasing
digital surveillance and cyber threats. Furthermore, researchers continue to explore
new techniques for detecting and blocking web trackers, which underscores the importance
of this field of research.

\section{Severity of web tracking} 

Web tracking has become a crucial issue in recent years, as it poses 
a serious threat to user privacy on the Internet. Englehardt et al. \cite{englehardt2016online}
conducted a comprehensive study on web tracking by crawling the top one
million websites and analyzing the presence of third-party trackers.
The study revealed that web tracking is pervasive, with more than 80\%
of the top websites containing trackers that collect users' online
behavior data. The data collected by these trackers can include personal
information, such as name, location, and browsing history. This study
underscores the urgent need for effective measures to prevent web
tracking and protect user privacy.

Schelter and Kunegis \cite{schelter2018ubiquity} also conducted an analysis
on the spread and dominance of web trackers, which confirmed the widespread
presence of trackers across the web. The study revealed that web trackers
are used by various companies, but are dominated by a few large ones,
such as \textit{Google}, \textit{Facebook}, and \textit{Twitter}. These companies are able
to track numerous users due to their extensive reach and
access to user data. This highlights the importance of regulating 
the use of web trackers and holding large companies accountable for their use.


In addition to the work of Englehardt et al. and Schelter and Kunegis,
several other studies have expressed privacy concerns about web trackers \cite{bujlow2017survey,chaabane2012big}.
For instance, Mayer and Mitchell \cite{mayer2012third} discuss the policy and technology 
challenges related to third-party web tracking, and argue that user consent
and control over personal data are critical. Nikiforakis et al. \cite{nikiforakis2013cookieless} explore
the ecosystem of web-based device fingerprinting. These studies emphasize the need for effective
web tracker blocking tools to protect user privacy.

The potential risks associated with web tracking are significant,
and include targeted advertising and the tracking of users' political
preferences, which can have serious implications for democratic
processes \cite{englehardt2015cookies, englehardt2016online, mayer2012third}. Web trackers can also be used to gather information
about users' personal interests, behaviors, and habits, which can be
exploited by malicious actors. The findings of these studies underscore
the importance of addressing the issue of web tracking and the need
for effective web tracker blocking tools.

In conclusion, the widespread presence of web trackers across the Internet
poses a serious threat to user privacy. The studies conducted
by Englehardt et al., Schelter and Kunegis, and other researchers
highlight the scale and severity of web tracking, and emphasize
the need for effective measures to prevent web tracking and
protect user privacy. It is crucial that policymakers, tech companies,
and individuals take action to regulate the use of web trackers
and ensure that user data is protected from unauthorized access and use.


\section{Tracking techniques}

Web tracking has become an integral part of the online ecosystem, enabling various 
entities to collect and analyze user data for a multitude of purposes. These methods
employ a range of techniques to track users' online activities, including browsing behavior,
interests, and preferences. To understand these techniques better, web tracking can be broadly
categorized into two types: stateless tracking and stateful tracking. In this section, we explore
the distinctions between these two approaches and their implications for user privacy and online experiences.

\subsection{Stateful Web Tracking}
Stateful web tracking is a pervasive phenomenon in the digital landscape that plays a crucial role in understanding user behavior,
personalizing online experiences, and enabling targeted advertising. Unlike stateless tracking, which relies on transient interactions,
stateful tracking involves the persistent storage of user-related information, allowing for long-term tracking and analysis. 

A common practice for this king of tracking involves the utilization \emph{cookies}. These cookies were primarily introduced to enable 
stateful web navigation \cite{jschwartz}. Cookies are small text files that are commonly used in web browsing to store information
on the user's computer or device. These files serve as a means for websites to remember specific data about a user,
allowing for enhanced browsing experiences and personalized content delivery. Cookies play a vital role in facilitating
various online activities, such as maintaining login sessions and remembering user preferences.

However, they can be abused to track users around the web. Cookies can be classified into two kinds.
The first-party cookie which are set by the visited website itself and third-party cookies which are set by external 
resources embedded within the visited website. These cookies allow the third party to store information on the client's browser in regard 
to the visited first-part website. In the case that this third-party resource is included on a large amount of websites it is possible 
for that third party to create a detailed browsing history and profile the user's browsing habits. This phenomenon is called \emph{third-party cookies}
and is one of the most common methods for web tracking. Similar functionality was adopted by other browser components like
\emph{Flash cookies}. 

Flash cookies, also referred to as Local Shared Objects (LSOs), emerged as a web tracking mechanism leveraging Adobe Flash technology.
Differing from conventional HTTP cookies, Flash cookies possessed the capability to store larger amounts of data, up to 100 kilobytes \cite{sipior2011online}.
Moreover, Flash cookies evaded the limitations and controls imposed upon regular cookies, offering a means to track user behavior,
store user preferences, and facilitate multimedia content on websites \cite{krishnamurthy2011privacy}.

However, the utilization of Flash cookies has witnessed a decline and is no longer extensively employed, primarily attributable
to privacy and data security concerns \cite{hoofnagle2010different}. Flash cookies, with their expanded storage capacity and detachment from browser controls,
allowed for the stealthy tracking of users across various browsers and devices without explicit consent or awareness,
thereby exacerbating privacy apprehensions \cite{soltani2010flash}.

The prevalence of mobile devices and the diminishing support for Adobe Flash have further contributed to the waning relevance of Flash
cookies. As modern browsers and mobile operating systems gradually phased out Flash technology, website developers transitioned to
alternative technologies like HTML5, rendering the reliance on Flash cookies for functionality less significant \cite{bbcflash}.

Furthermore, \emph{HTML5}, the latest version of the HyperText Markup Language, introduces novel mechanisms that enable websites and
third-party trackers to store data within the browser environment without the need for traditional cookies. One such mechanism
is the utilization of the \emph{Window.name} property in \emph{JavaScript}, which serves as a non-persistent storage medium for sharing
information across different websites. The \emph{Window.name} property allows for the exchange of data between domains,
even in scenarios where the domains are distinct \cite{sy2018tracking}. By leveraging this property, websites and third-party trackers
can persistently store data within the browser, effectively bypassing the limitations associated with traditional
cookie-based storage \cite{pan2015not}. The \emph{Window.name} property, in conjunction with HTML5's enhanced functionality, provides a
flexible and efficient approach to data storage and sharing within the web ecosystem.

The use of third-party cookies is a common topic in the research community \cite{mayer2012third,
roesner2012detecting, nikiforakis2013cookieless, west2012analysis,krishnamurthy2006generating,englehardt2015cookies}.
Goel et al. \cite{goel2010anatomy} examined the usage of third-party cookies on popular websites and revealed that more than
80\% of the analyzed websites had at least on third-party cookie. Mayer and Mitchell \cite{mayer2012third} found that around
85\% of websites in their dataset utilized third-party cookies for tracking users. Furthermore, Englehardt et al.
\cite{englehardt2016online} performed a comprehensive study examining the prevalence of online tracking mechanisms, including third-party cookies. Their results indicate 
that approximately 94\% of the studied websites use third-party cookies for tracking purposes,
further underscoring their pervasive presence.

Furthermore, the public has been informed about third-party tracking due to the "What they know" series by the Wall Street Journal
in 2010 \cite{whattheyknow}. This seminal series served as a catalyst for society's recognition of the issue, leading to a surge in public concern for online privacy. Responding to this mounting pressure, the Electronic Frontier Foundation (EFF)
introduced the Do Not Track (DNT) policy \cite{dnt}. DNT operates as a header field that enables users to express
their preferences regarding tracking practices. By activating the DNT flag, users explicitly indicate their
desire to avoid being tracked without clear and informed consent. This policy has been widely adopted by various web browsers,
making it accessible to users through their browser settings. Moreover, the private browsing mode was created allowing users to automatically delete the local 
storage after each browsing session. Besides, browser extensions were developed which block third-party tracking to an extent \cite{ghostery,abp,trackingObserver, shareMeNot}.

\subsection{Stateless Web Tracking}
Stateless web tracking refers to a category of web tracking techniques that do not rely on storing user data on
the client-side or maintaining persistent identifiers. This stateless identification of devices is a technique used
in web tracking that involves collecting and analyzing various attributes of a user's device to create a unique identifier.
These attributes can include information about the device's operating system, browser version, screen resolution,
installed fonts, and plug-ins, among others. By combining these attributes, a fingerprint is generated that can be used to track and
identify a user across different websites and sessions, even if cookies are cleared or disabled or the user is browsing in a private 
browsing window.

These fingerprints are created by accessing all available information about the user's device with JavaScript which runs in the browser.
JavaScript provides many features interacting with the resources of the computer, because of that it is possible to identify the user's 
operating system and architecture. Furthermore, it is possible to get browser specific information via the \emph{"navigator"} object as well as 
screen information like the \emph{"width"}, \emph{"height"} and \emph{"color depth"} of the screen via the \emph{"screen"} object. All the available 
information can then be hashed and used to identify a certain user. In addition, it is easier to distribute such hashes because they do not depend 
on information stored on the user's device such as cookies. Any third-party party which hashes with the same information and the same algorithm 
can identify a user another tracking service identified previously.

Using a combination of available fields, namely \emph{"navigator"}, \emph{"screen"}, \emph{"navigator.plugins"}, and \emph{"navigator.MimeTypes"},
Mayer \cite{mayer2009any} successfully applied a hashing technique to achieve a remarkable 96\% unique identification rate among
a sample of 1,328 users.

Eckersley \cite{eckersley2010unique} conducted a study to evaluate the uniqueness and stability of
device fingerprints. The results indicated that device fingerprints can be highly unique,
with a low probability of collision even among a large population of devices. The study also found
that device fingerprints can remain stable over time, making them reliable for long-term tracking purposes.

Mowery et al. \cite{mowery2011fingerprinting} investigated the prevalence of device fingerprinting
in the wild by analyzing a large dataset of websites. Their findings revealed that a significant
number of websites engage in device fingerprinting activities, further highlighting its widespread
use as a tracking mechanism.

However, these features are not the only features available. It was also possible to determine different JavaScript engines by 
performing benchmarks on the user's device \cite{miyazaki2008online}, or to exploit the \emph{canvas} API to determine differences 
in the HTML rendering also known as canvas fingerprinting \cite{mowery2012pixel}.

Furthermore, the usage of \emph{web beacons} is another stateless tracking techniques.
Web beacons, also known as tracking pixels or clear GIFs, are invisible elements embedded within web content.
These small, transparent images or code snippets allow website operators and third-party trackers to monitor
and analyze user behavior on the internet.

They operate by initiating a request to a remote server when a user accesses a webpage
or interacts with specific online content that contains the beacon. This request includes various
information such as the user's IP address, browser type, referring webpage, and the time of
the interaction. By tracking the delivery and response of these requests, website operators
and third-party trackers can gain valuable insights into users' online activities,
including page views, click-through rates, and the effectiveness of marketing campaigns \cite{zimmer2010but}.

Web beacons are commonly utilized in email tracking as well. When an email contains a web beacon, it allows the sender to monitor
if and when the recipient opens the email, as well as track other engagement metrics \cite{gurses2011engineering}. 
Moreover, web beacons play a significant role in online advertising by enabling advertisers to measure the effectiveness
of their campaigns and target specific audiences. A study by Nikiforakis et al. \cite{nikiforakis2013cookieless}
explored the presence of web beacons in online advertisements and found that approximately 45\% of the analyzed advertisements
utilized web beacons for tracking user interactions. This indicates the widespread usage of web beacons in the advertising
industry to monitor user engagement and optimize ad delivery.

The extensive usage of web beacons raises concerns about user privacy and tracking practices. As web beacons operate invisibly,
users may be unaware of their presence and the extent of data collection associated with them. Furthermore, the ability to track
users across multiple websites and platforms through web beacons can lead to the creation of detailed user profiles,
potentially infringing upon privacy rights \cite{acquisti2015privacy}.

\section{Types of Tracking}
This section delves into two primary categories of tracking: first-party tracking and third-party tracking.
Each category encompasses distinct applications, and considerations, shedding light on the complexities
of user data collection and its impact on privacy, personalization, and user experiences.
\subsection{First-party Tracking}

First-party tracking refers to the collection and analysis of user data by the website or domain that the user directly
interacts with. This type of tracking involves the website owner using various technologies and techniques to monitor user
behavior, personalize experiences, and improve their own services. First-party tracking can encompass a wide range of
possibilities and applications, which are crucial for understanding user preferences, enhancing website usability,
and delivering targeted content.
\begin{enumerate}
  \item{\textbf{Personalization and Customization:} First-party tracking allows websites to personalize user experiences based on their browsing history,
    preferences, and demographics. By monitoring user interactions, websites can tailor content,
    recommendations, and suggestions to individual users. For instance, online retailers can use first-party
    tracking to offer personalized product recommendations. Such personalization enhances user engagement
    and satisfaction by providing relevant and targeted content.
    }
  \item{\textbf{Analytics and Performance Optimization:} First-party tracking enables website owners to gather data about user behavior and website performance.
    By analyzing metrics like page views, click-through rates, and conversion rates, website owners can gain
    insights into user engagement and make data-driven decisions to optimize their website.
    }
  \item{\textbf{Security and Fraud Prevention:} First-party tracking can be employed to enhance website security and detect fraudulent
    activities. By monitoring user behavior patterns, websites can identify suspicious activities, such as account takeover
    attempts or unusual transactions, and trigger security measures to protect users and their data.
    First-party tracking can also aid in preventing click fraud and bot activity.
  }
  \item{\textbf{User Authentication and Session Management:} First-party tracking is instrumental in managing user sessions and authentication.
    Websites use techniques like session cookies or local storage to maintain user sessions and enable secure logins.
    By tracking user authentication status, websites can provide personalized experiences and maintain user preferences across sessions.
  }
  \item{\textbf{A/B Testing and Conversion Optimization:} First-party tracking plays a crucial role in conducting A/B testing and
    conversion rate optimization. Websites can track user interactions and behavior on different versions of their webpages
    to analyze the impact of design or content variations on user engagement and conversions. This data-driven approach
    helps optimize website elements for better performance \cite{siroker2015b}.
  }
\end{enumerate}
These examples showcase the diverse applications of first-party tracking in enhancing user experiences, improving website performance,
and ensuring security. It is important to note that the implementation and practices of first-party is an ever-growing environment and the 
provided use cases are not comprehensive.
\subsection{Third-party Tracking}
Third-party tracking involves the collection and analysis of user data by entities separate from the website or domain directly
interacted with by the user. It encompasses the monitoring and tracking activities conducted by third-party trackers, often
for advertising and analytics purposes.
\begin{enumerate}
  \item{\textbf{Targeted Advertising:} One prominent application of third-party tracking is targeted advertising.
      Third-party trackers collect user data from multiple websites to create detailed profiles and deliver personalized
      advertisements based on users' browsing history, preferences, and demographics \cite{agarwal2013not,acquisti2016economics,turow2009americans}. This form of tracking
      enables advertisers to reach their target audiences more effectively and increase the relevance of advertisements displayed.
    }
  \item{\textbf{Cross-Site Analytics:} Third-party tracking facilitates the aggregation of user data across different websites
      to generate comprehensive analytics and insights. Analytics platforms and data brokers employ third-party tracking techniques
      to collect data on user behavior, engagement metrics, and conversion rates across various websites \cite{googleAna}.
      These insights can then be used to optimize marketing strategies, improve user experiences, and inform business decisions.
    }
  \item{\textbf{Data Monetization:} Third-party tracking plays a pivotal role in the monetization of user data. Data brokers and
      other intermediaries collect and aggregate user data from multiple sources, including third-party tracking, and sell this
      data to interested parties such as marketers, advertisers, and researchers \cite{turow2015tradeoff}. The data obtained through
      third-party tracking can be valuable for market research, consumer profiling, and trend analysis.
    }
\end{enumerate}

Third-party tracking is a lucrative operation to generate higher conversion rates or to sell the collected data to other companies.
The value of the data is depended on the retrieved information. Mayer and Mitchell \cite{mayer2012third} conducted an investigation into the landscape of third-party
web tracking, highlighting the role of cross-site tracking in enabling comprehensive user profiling.
They emphasized the potential privacy risks associated with cross-site tracking and the challenges
it poses to user control over their personal information. The tracking activities conducted by third-party
entities often occur without users' explicit knowledge or consent, leading to debates about
transparency and control over personal data.
\section{Tracking Protection Lists}

As mentioned before, tracking poses a significant challenge to online privacy, necessitating the development of effective countermeasures
to mitigate its impact. One prevalent approach in combating this issue involves the use of Tracking Protection Lists (TPLs),
commonly referred to as \textit{"blocking lists"} or \textit{"blacklists"} \cite{bujlow2017survey,mayer2012third}. TPLs consist of carefully curated collections of domain names
that are classified by the community. These lists are instrumental in blocking third-party content or services and serve two
primary purposes. Firstly, they can be implemented at the router level, enabling the direct blocking of malicious domains.
Secondly, they can be integrated into web browsers, which is the more commonly adopted method due to its simplicity in terms of setup.

To facilitate the blocking or limitation of third-party access, various client-side tools have been developed, offering users greater
control over their online privacy. Prominent examples of such tools include \textit{Ghostery}, \textit{AdBlockPlus}, \textit{TrackingObserver}, and \textit{ShareMeNot}
\cite{ghostery, abp, trackingObserver, shareMeNot}.
Studies have demonstrated the positive privacy effects of these tools, underscoring their efficacy in mitigating third-party tracking \cite{kontaxis2015tracking}.

However, it is important to acknowledge the limitations associated with TPLs. The frequent updates and accurate labeling of domains are crucial
to ensure their effectiveness. Failure to maintain up-to-date and properly labeled lists may result in certain third-party entities evading detection
by the blocking tools or causing malfunctions in web pages. This challenge has prompted researchers to explore alternative approaches to enhance
the maintenance and updating process of blocking lists.

In light of this, a promising avenue of research in the field of web tracking countermeasures involves the utilization of Machine Learning (ML)
models. These models offer the potential to automate and streamline the process of managing and updating blocking lists, addressing the inherent
challenges associated with manual curation. By leveraging ML algorithms, it becomes possible to analyze vast amounts of data,
identify patterns, and make accurate predictions about the domains that should be included in the blocking lists. This not only enhances the efficiency
and accuracy of the lists but also reduces the burden on users and administrators in terms of manual maintenance and updates.

\section{Machine Learning Based Tracking Detection}

The utilization of machine learning methods in order to detect and/or block web tracker go back to 1999. In this year, Kushmerick \cite{kushmerick1999learning}
introduced a ML based advertisement blocker. This approach utilizes inductive learning to automatically
generate rules that enable the blocker to distinguish between ads and non-ads. By removing advertisements before they are downloaded,
the blocker effectively conserves bandwidth. This is achieved through the extraction of relevant features that aid in determining
whether a given element is an ad, such as analyzing the size of images. It is worth noting that the production implementation of
this blocker requires approximately 70 milliseconds to remove each image. The learned rules in this ML-based approach yield an
impressive accuracy of 97.1\%. A similar approach to block unwanted images from loading was published in 2005 by Esfandiari \cite{esfandiari2005adaptive}.
They used a different algorithm to prevent the unwanted images from loading and also achieved great success with minimal user feedback.

Nowadays, there is shift from detecting advertisements to detecting and blocking unwanted web tracker. As mentioned before 
the kind of tracker can be categorized as stateful tracker and stateless tracker. Therefore, we split the machine learning based
protection approaches into these two categories.

\subsection{Stateful Tracking Detection}

One of the first ML based approaches to stateful web tracking was published by Yamanda et al. \cite{yamada2010web} in 2010.
In their approach they analyzed traffic at the network gateway and constructing a graph for the traffic by monitoring the visited
time on each website and the website itself. They then labeled a dataset by utilizing public available tracking protection lists
and trained their supervised classifier with this data. Their classifier achieved an accuracy between 62\% to 73\%.

Furthermore, Li et al. \cite{li2015trackadvisor} presented \emph{"TrackAdvisor"} which utilizes machine learning for 
accurately identifying HTTP requests carrying sensitive information to third-party trackers. It achieved a recall rate of 100\% and a precision rate of 99.4\%, outperforming
existing third-party tracking blacklists. The study found that 46\% of the websites in the Alexa Global Top 10,000 had at least
one third-party tracker, with Google monitoring approximately 25\% of these popular websites. The research aimed to measure
the pervasiveness of third-party tracking and raise awareness about its privacy risks.

Additionally, Metwalley et al. \cite{metwalley2015unsupervised} published an unsupervised methodology that utilizes application-level traffic logs to automatically
detect services engaged in tracking activities. The methodology is based on an algorithm that identifies user identifiers exposed
in URL queries within HTTP(S) transactions. The algorithm was validated using an artificial dataset created by visiting the top
200 most popular websites according to the Alexa rank. The results were promising, with the algorithm identifying 34 previously
unknown third-party trackers not found in existing blacklists. The analysis of the algorithm's output revealed privacy-related
interactions, including scenarios suggesting the practice of Cookie Matching, where user activity information is shared across
multiple third-party entities.

A similar study has been performed by Dudykevych \cite{dudykevych2016detecting} in which the authors aimed to address privacy violations caused by third-party services and evaluate the effectiveness
of a method for automatically identifying web-tracking requests. By analyzing a corpus of internet traffic, they were able
to detect 95\% of third-party trackers, including previously unseen trackers. The results demonstrated the potential of
their approach to accurately identify web trackers based on HTTP request analysis. The study highlighted the significance
of shared cookies across different domains, shedding light on the extent of privacy concerns associated with third-party tracking.

In 2023, Raschke et al. \cite{raschke2023} introduced \emph{"t.ex-Graph"}, which is a representation that models the flow of data
between websites in the form of a graph to identify Web trackers. They utilized a publicly available dataset
of HTTP/S requests collected from the Tranco top 10K websites to extract the graph. Additionally, they defined a set of features
which were used to train ML models on the dataset. The experimental results demonstrate an impressive accuracy rate of 88\% and the capability to identify previously unknown Web trackers.

The previous shown examples can all be used to validate TPLs, but a preemptive blocking of requests in real-time seams difficult, because some 
of the discussed features can only be obtained after the requests has been sent to host or the model requires a lot of information in order 
to perform its classification. However, Castell-Uroz et al. \cite{castell2020url} proposed a method that encodes the URL of a request and passes this
encoded representation to a classifier. This is a quite reduced approach to this topic and requires only data which is available before a request
is sent. They achieved an accuracy of 97\% and state the model could be applied in real-time scenarios.

\subsection{Stateless Tracking Detection}

A similar effort to develop ML based tracking detection is performed in the stateless tracking detection field. Bau et al. \cite{bau2013promising} 
investigated the use of machine learning (ML) for tracker detection. Their research focused on the advantages of ML,
including ease of maintenance and the ability to adapt to changing tracking behaviors. They collected data from a web
crawl of the Quantcast United States top 32,000 homepages, labeling each node in DOM-like hierarchies. After training the ML model
on this dataset, they achieved a weighted precision of 96.7\% with a false-positive rate of 0.5\%.

Kaizer et al. \cite{kaizer2016towards} investigated machine-based tracking, focusing specifically on JavaScript-oriented tracking due to its widespread
accessibility. They explored the use of coarse features related to JavaScript access, cookie access, and URL length subdomain
information to create a classifier for identifying machine-based trackers. The classifier achieved an accuracy of 97.7\%.
The researchers applied this classifier to real-world datasets obtained from 30-minute website crawls, including websites targeting
children and those targeting a popular audience. They found that over 85\% of all websites, including those targeting regulated groups
like children, utilized machine-based tracking.

In the same year, Wu et al. \cite{wu2016machine} addressed the problem of privacy violations caused by third-party tracking and the
limitations of blacklist-based defense methods. They proposed \emph{"DMTrackerDetector"}, a system that automatically detected third-party
trackers with high accuracy. By leveraging the structural hole theory to preserve first-party trackers and employing supervised
machine learning, they achieved a 97.8\% detection accuracy for third-party trackers. The generated blacklist covered the popular
Ghostery \cite{ghostery} list and unveiled 35 previously unknown trackers. This research contributed to improving the efficiency
and coverage of anti-tracking technologies.

Ikram et al. \cite{ikram2016towards} took a different approach to this problem. In their work they focus on the usability aspect
in which more blocking tools reduce the user experiences on websites \cite{leon2012johnny}. They recognized the shared similarities in tracking
JavaScript code, attributed to the use of common libraries by developers. To validate their assumption, they trained a one-class
classifier on semantic and syntactic features of JavaScript code. The classifier, trained on 2,612 manually labeled JavaScript
programs, achieved an impressive accuracy of 99\%. The research also
revealed that their model exhibited different labeling patterns compared to other tools or blocking lists. Manual investigation
confirmed that the model not only enhanced the user experience but also uncovered new tracking services. However, the method
faced challenges in data collection and its adaptability to library changes.

Iqbal et al. \cite{iqbal2018adgraph} made significant progress in ML-based web tracker detection by integrating multiple layers of information
into a unified supervised learning model. They structured browsing data as a directed graph with three node types
(HTML, HTTP, JavaScript) and edges representing browser events. The ML model extracted features (e.g., in-degree, out-degree, descendants)
for each node and labeled data points as advertisement or no-advertisement. The absence of a ground truth for tracker detection
posed challenges, as filter lists contained false negatives and false positives. To address this, Iqbal et al. (2023) established 
a methodology for handling divergent classifications. Despite these complexities, their approach yielded impressive results,
achieving 97.7\% accuracy and 83\% precision. Notably, their strict evaluation methodology accounted for discrepancies between
the model and blocking lists, further underscoring the significance of their high accuracy.

In their subsequent work, Iqbal et al. \cite{iqbal2020adgraph} conducted an evaluation of their \emph{"ADGRAPH"} \cite{adgraph} machine learning (ML) model
using the top 10,000 web pages from the Alexa ranking. They employed a customized Chromium browser for website
crawling enabling the construction of a directed graph spanning HTML, HTTP, and
JavaScript layers. Structural features (e.g., graph size, degrees, number of siblings) capturing relationships between web
elements and content features (e.g., request type, keywords, URL length) representing values associated with individual
nodes were extracted from this graph. These features were then input into their trained classifier to determine tracking
versus non-tracking attributes. The analysis of the crawl results confirmed that non-ad nodes exhibit higher average
degree connectivity as they interact with web page content, while ad nodes show lower average degree connectivity.
Additionally, the study demonstrated a 95.33\% replication of labels from human-generated blocking lists and identified errors
in these lists.

Similar to Iqbal et al. \cite{iqbal2020adgraph}, Rizzo et al. \cite{rizzo2021unveiling} developed and
evaluated a methodology combining JavaScript code analysis and machine learning to detect web fingerprinters.
Using a dataset of over 400,000 JavaScript files accessed during a month-long experiment, they compared static
and dynamic code analysis approaches to uncover fingerprinting practices. Their methodology achieved 94\% accuracy
with fast decision time, identifying 840 fingerprinting services, including 695 previously unknown to popular tracker
blockers. These findings contribute to the understanding and detection of web fingerprinting techniques.


\section{Research gap}

As previously discussed, many of developed classifier are not designed for real-time applications. Most of them can only
be used to evaluate TPLs. For stateful tracker detection the most promising classifier with regard to real-time application is
Castell-Uroz et al. \cite{castell2020url}. With the rise of client-side ML libraries such as \emph{"tensorflowjs"} the goal of
real-time tracker detection and even the blocking of these requests comes closer to achieve.

Furthermore, there are no real comparisons regarding the use of a real-time classifier and the usage of tracking blockers. This could
generate more insights about the ML classifiers and demonstrate the real world application of the resarch approaches.

